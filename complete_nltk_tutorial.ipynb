{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd05e2c-82bc-4cf2-bdca-114ad6c5c9a1",
   "metadata": {},
   "source": [
    "# TEXT PREPROCESSING PIPELINE (MOST IMPORTANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc6b11f3-dc93-47a5-856f-290ba1e8c95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/e2727bef-27c9-47fa-850c-\n",
      "[nltk_data]     5006ef847018/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/e2727bef-27c9-47fa-850c-\n",
      "[nltk_data]     5006ef847018/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/e2727bef-27c9-47fa-850c-\n",
      "[nltk_data]     5006ef847018/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                # The main NLP library\n",
    "import re                  # Regular Expressions - for finding patterns in text\n",
    "from nltk.corpus import stopwords    # Common words like \"the\", \"is\", \"and\"\n",
    "from nltk.stem import WordNetLemmatizer  # Converts words to base form\n",
    "\n",
    "# Downloads needed data\n",
    "nltk.download('punkt')      # Tokenizer (splits text into words)\n",
    "nltk.download('stopwords')  # List of common words to remove\n",
    "nltk.download('wordnet')    # Dictionary for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bca86199-598f-4930-97b2-06d36cdf2f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned tokens: ['check', 'tutorial', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Real-world text cleaning pipeline\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # 2. Remove URLs, mentions, hashtags (for social media)\n",
    "    text = re.sub(r'http\\S+|@\\w+|#\\w+', '', text)\n",
    "\n",
    "    # 3. Remove numbers and special chars (keep words only)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # 4. Tokenize\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # 5. Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # 6. Lemmatization (better than stemming for DS)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # 7. Remove short words (<2 chars)\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# Test it\n",
    "sample = \"Check out this tutorial at https://example.com! #NLP is awesome 100%\"\n",
    "print(\"Cleaned tokens:\", preprocess_text(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bf3a92-0020-468a-a7dc-8c085dd3c2c8",
   "metadata": {},
   "source": [
    "# FEATURE EXTRACTION (For ML Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7371ead-a421-47c8-b7dc-5e3455953ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/e2727bef-27c9-47fa-850c-\n",
      "[nltk_data]     5006ef847018/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /home/e2727bef-27c9-47fa-850c-\n",
      "[nltk_data]     5006ef847018/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from collections import Counter\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "296a0c07-f9ad-4200-88d7-5cf2573638f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1 features: {'word_count': 4, 'char_count': 36, 'avg_word_length': 7.75, 'noun_ratio': 0.25, 'verb_ratio': 0.5, 'unique_word_ratio': 1.0}\n",
      "Review 2 features: {'word_count': 4, 'char_count': 41, 'avg_word_length': 6.5, 'noun_ratio': 0.25, 'verb_ratio': 0.25, 'unique_word_ratio': 1.0}\n",
      "Review 3 features: {'word_count': 3, 'char_count': 27, 'avg_word_length': 6.0, 'noun_ratio': 0.6666666666666666, 'verb_ratio': 0.0, 'unique_word_ratio': 1.0}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features(text_list):\n",
    "    \"\"\"\n",
    "    Convert text to ML-ready features\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    for text in text_list:\n",
    "        tokens = preprocess_text(text)\n",
    "        \n",
    "        # Feature 1: Basic counts\n",
    "        features_dict = {\n",
    "            'word_count': len(tokens),\n",
    "            'char_count': len(text),\n",
    "            'avg_word_length': sum(len(w) for w in tokens)/len(tokens) if tokens else 0   #Average letters per word -> [\"cat\", \"elephant\"] â†’ (3+8)/2 = 5.5\n",
    "        }\n",
    "        \n",
    "        # Feature 2: Word frequencies (top 10)\n",
    "        freq_dist = FreqDist(tokens)\n",
    "        top_words = freq_dist.most_common(5)\n",
    "        \n",
    "        # Feature 3: POS ratios\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        pos_counts = Counter(tag for word, tag in tagged)\n",
    "        \n",
    "        if len(tokens) > 0:\n",
    "            features_dict.update({\n",
    "                'noun_ratio': pos_counts.get('NN', 0)/len(tokens),\n",
    "                'verb_ratio': sum(pos_counts.get(tag, 0) for tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])/len(tokens),\n",
    "                'unique_word_ratio': len(set(tokens))/len(tokens)\n",
    "            })\n",
    "        \n",
    "        features.append(features_dict)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Example\n",
    "reviews = [\n",
    "    \"Amazing product! Highly recommended.\",\n",
    "    \"Terrible experience, would not buy again.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "features = extract_features(reviews)\n",
    "for i, feat in enumerate(features):\n",
    "    print(f\"Review {i+1} features: {feat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505fe4c-bd4b-438b-88e8-4d9891c80719",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS (Most Common DS Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91ab92b3-4867-40a4-80ea-e505efbfb2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/e2727bef-27c9-47fa-850c-\n",
      "[nltk_data]     5006ef847018/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')  # VADER stands for: Valence Aware Dictionary and sEntiment Reasoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "187fb7e0-0212-4f2d-a8ed-a5cf1b3c9ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POSITIVE   | Score: 0.659 | The product works great and delivery was fast!...\n",
      "NEGATIVE   | Score: -0.852 | Poor quality, broke after 2 days. Very disappointe...\n",
      "POSITIVE   | Score: 0.649 | It's okay for the price, but could be better....\n",
      "POSITIVE   | Score: 0.903 | Absolutely love it! Best purchase ever!!!...\n"
     ]
    }
   ],
   "source": [
    "def analyze_sentiments(texts):\n",
    "    \"\"\"\n",
    "    Analyze positive/negative sentiment\n",
    "    \"\"\"\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    results = []\n",
    "    \n",
    "    for text in texts:\n",
    "        scores = sia.polarity_scores(text)\n",
    "        # Compound score: -1 (very negative) to +1 (very positive)\n",
    "        sentiment = \"positive\" if scores['compound'] > 0.05 else \"negative\" if scores['compound'] < -0.05 else \"neutral\"\n",
    "        \n",
    "        results.append({\n",
    "            'text': text[:50] + \"...\",  # Preview\n",
    "            'sentiment': sentiment,\n",
    "            'compound_score': scores['compound'],\n",
    "            'pos_score': scores['pos'],\n",
    "            'neg_score': scores['neg']\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Real-world example\n",
    "customer_reviews = [\n",
    "    \"The product works great and delivery was fast!\",\n",
    "    \"Poor quality, broke after 2 days. Very disappointed.\",\n",
    "    \"It's okay for the price, but could be better.\",\n",
    "    \"Absolutely love it! Best purchase ever!!!\"\n",
    "]\n",
    "\n",
    "sentiments = analyze_sentiments(customer_reviews)\n",
    "for result in sentiments:\n",
    "    print(f\"{result['sentiment'].upper():10} | Score: {result['compound_score']:.3f} | {result['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d7155-9151-4d67-8863-f89e1d46aee5",
   "metadata": {},
   "source": [
    "# TOPIC MODELING (LDA - Super Useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d884023b-9a38-4a97-8b0e-da6e1d2fbb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š We have 6 documents\n",
      "ðŸ“ Sample processed: stock market reach time high tech company soar...\n",
      "âœ… Created matrix: 6 docs Ã— 41 words\n",
      "Topic 1: company | global | soar | recession | stock\n",
      "Topic 2: vision | show | reserve | hike | federal\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "def extract_topics(documents, n_topics=3, n_words=5):\n",
    "    \"\"\"\n",
    "    Discover hidden topics in documents - FIXED VERSION\n",
    "    \"\"\"\n",
    "    # 1. Preprocess all documents\n",
    "    processed_docs = [' '.join(preprocess_text(doc)) for doc in documents]\n",
    "    \n",
    "    print(f\"ðŸ“Š We have {len(documents)} documents\")\n",
    "    print(f\"ðŸ“ Sample processed: {processed_docs[0][:50]}...\")\n",
    "    \n",
    "    # 2. Create document-term matrix - WITH FIXED PARAMETERS!\n",
    "    vectorizer = CountVectorizer(\n",
    "        max_df=0.95,      # Remove words in >95% of docs (too common)\n",
    "        min_df=1,         # CHANGED FROM 2 TO 1! (keep words that appear at least once)\n",
    "        max_features=100  # CHANGED FROM 1000! (for small dataset)\n",
    "    )\n",
    "    \n",
    "    dtm = vectorizer.fit_transform(processed_docs)\n",
    "    \n",
    "    print(f\"âœ… Created matrix: {dtm.shape[0]} docs Ã— {dtm.shape[1]} words\")\n",
    "    \n",
    "    # 3. Check if we have enough words\n",
    "    if dtm.shape[1] < n_topics:\n",
    "        print(f\"âš ï¸  Warning: Only {dtm.shape[1]} words for {n_topics} topics\")\n",
    "    \n",
    "    # 4. Apply LDA\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_components=min(n_topics, dtm.shape[1]),  # Safety: topics â‰¤ words\n",
    "        random_state=42,\n",
    "        learning_method='online'\n",
    "    )\n",
    "    \n",
    "    lda.fit(dtm)\n",
    "    \n",
    "    # 5. Display topics\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_words_idx = topic.argsort()[:-n_words-1:-1]\n",
    "        top_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append(f\"Topic {topic_idx+1}: {' | '.join(top_words)}\")\n",
    "    \n",
    "    return topics\n",
    "\n",
    "# Example with news headlines\n",
    "headlines = [\n",
    "    \"Stock market reaches all time high as tech companies soar\",\n",
    "    \"New AI model achieves breakthrough in language understanding\",\n",
    "    \"Federal Reserve considers interest rate hike to curb inflation\",\n",
    "    \"Deep learning algorithms revolutionize medical diagnosis\",\n",
    "    \"Economists predict recession amid global uncertainties\",\n",
    "    \"Neural networks show remarkable progress in computer vision\"\n",
    "]\n",
    "\n",
    "topics = extract_topics(headlines, n_topics=2)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc4496-db87-4e62-8fc2-c0f9f208e148",
   "metadata": {},
   "source": [
    "# TEXT CLASSIFICATION (Complete Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ab328e2-ab3f-4b70-875a-50a3f1142f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7cb0c5c1-7e76-4a77-b297-ff80a77f7523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      1.00      1.00         1\n",
      "        spam       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n",
      "\n",
      "New message prediction: spam\n"
     ]
    }
   ],
   "source": [
    "def train_text_classifier(texts, labels, test_size=0.2):\n",
    "    \"\"\"\n",
    "    End-to-end text classifier\n",
    "    \"\"\"\n",
    "    # 1. Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=test_size, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 2. Feature extraction (TF-IDF)\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)  # Include bigrams\n",
    "    )\n",
    "    \n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "    \n",
    "    # 3. Train classifier\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train_vec, y_train)\n",
    "    \n",
    "    # 4. Evaluate\n",
    "    y_pred = classifier.predict(X_test_vec)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # 5. Return everything for reuse\n",
    "    return {\n",
    "        'vectorizer': vectorizer,\n",
    "        'classifier': classifier,\n",
    "        'accuracy': classifier.score(X_test_vec, y_test)\n",
    "    }\n",
    "\n",
    "# Example: Spam vs Ham classification\n",
    "messages = [\n",
    "    \"Win a free iPhone! Click here now!\",\n",
    "    \"Meeting scheduled for tomorrow at 3 PM\",\n",
    "    \"Congratulations! You've won $1000 prize!\",\n",
    "    \"Don't forget to submit your report by Friday\",\n",
    "    \"Limited time offer! Buy one get one free!\",\n",
    "    \"Can we reschedule our call to next week?\"\n",
    "]\n",
    "\n",
    "labels = ['spam', 'ham', 'spam', 'ham', 'spam', 'ham']  # Corresponding labels\n",
    "\n",
    "model = train_text_classifier(messages, labels)\n",
    "\n",
    "# Test with new message\n",
    "new_message = [\"Urgent! Claim your prize now!\"]\n",
    "new_vec = model['vectorizer'].transform(new_message)\n",
    "prediction = model['classifier'].predict(new_vec)\n",
    "print(f\"\\nNew message prediction: {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fec11cf-2da4-4e93-b5db-c98c96de3787",
   "metadata": {},
   "source": [
    "# NAMED ENTITY RECOGNITION (For Information Extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c650bc99-def3-4b23-a887-d3eb8bd9ab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/e2727bef-27c9-47fa-850c-\n",
      "[nltk_data]     5006ef847018/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /home/e2727bef-27c9-47fa-850c-\n",
      "[nltk_data]     5006ef847018/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "21d36a10-2088-43ca-b1de-9f523c25b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Named Entities Found:\n",
      "PERSON: Elon Musk, Microsoft, Apple\n",
      "ORGANIZATION: CEO Tim Cook, SpaceX\n"
     ]
    }
   ],
   "source": [
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extract people, organizations, locations\n",
    "    \"\"\"\n",
    "    # Tokenize and POS tag\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    # Apply NER\n",
    "    chunks = ne_chunk(tagged)\n",
    "    \n",
    "    entities = {\n",
    "        'PERSON': [],\n",
    "        'ORGANIZATION': [],\n",
    "        'LOCATION': []\n",
    "    }\n",
    "    \n",
    "    # Extract entities\n",
    "    for chunk in chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            entity_type = chunk.label()\n",
    "            entity_name = ' '.join(c[0] for c in chunk)\n",
    "            \n",
    "            if entity_type in entities:\n",
    "                entities[entity_type].append(entity_name)\n",
    "    \n",
    "    return entities\n",
    "\n",
    "# Example from news article\n",
    "news_text = \"\"\"\n",
    "Apple CEO Tim Cook announced new products at the event in California.\n",
    "Microsoft and Google are also investing heavily in AI research.\n",
    "Elon Musk's SpaceX launched satellites from Florida last week.\n",
    "\"\"\"\n",
    "\n",
    "entities = extract_entities(news_text)\n",
    "print(\"ðŸ“Œ Named Entities Found:\")\n",
    "for entity_type, names in entities.items():\n",
    "    if names:\n",
    "        print(f\"{entity_type}: {', '.join(set(names))}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-panel-2023.05-py310",
   "language": "python",
   "name": "conda-env-anaconda-panel-2023.05-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
